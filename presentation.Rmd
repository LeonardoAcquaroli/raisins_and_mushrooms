---
title: "Presentation"
author: "Casual deep learning"
date: "2023-06-13"
output:
  html_document:
    keep_md: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# Description

Raisins can be of two species: *Kecimen* and *Besni*.\
We built different models to predict the species on the basis of the dimensions of the raisin example approximated as an ellipse.

## Variables

The predictors that can be used are:

-   Area MajorAxisLength
-   MinorAxisLength
-   Eccentricity
-   ConvexArea
-   Extent
-   Perimeter
-   Class_literal

# Descriptive Analysis

#### Loading Libraries

```{r}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(glmnet)
library(estimatr)
library(stats)
library(maxLik)
library(Matrix)
library(caret)
library(performance)
library(see)
library(corrplot)
library(GGally)
library(car)
library(FactoMineR)
library(factoextra)
library(dplyr)
library(e1071)
library(caTools)
library(class)
library(pROC)
```

#### Loading data-set

(Aim: visualize dataset, summary, remove Class_literal)

```{r}
raisins = read.csv(
  "https://raw.githubusercontent.com/LeonardoAcquaroli/raisins_and_mushrooms/main/datasets/Raisin_Dataset.csv",
  sep = ";"
)

head(raisins)

# remove the Class_literal  column  
raisins_corr <- raisins
raisins <- raisins[,-8] 
# Class: 1 if raisin is of Kecimen type, 0 if it is Besni
```

### Correlations plots

(Aim: visually represent relationships between variables)

```{r, echo=FALSE}
cor_table<-cor(raisins) 

corrplot(cor_table, type = "upper",     #first corr plot
         tl.col = "black", tl.srt = 45)

ggcorr(raisins, method = c("everything", "pearson")) #heatmap plot

ggpairs(raisins_corr, columns = 1:7, ggplot2::aes(colour= Class_literal, alpha = 0.005)) #cor by groups
```

### Boxplot

(Aim: Identify outliers)

```{r, echo=FALSE}
boxplot(raisins$Area, xlab = "Area")
boxplot(raisins$MajorAxisLength, xlab = "MajorAxisLength")
boxplot(raisins$MinorAxisLength, xlab = "MinorAxisLength")
boxplot(raisins$Eccentricity, xlab = "Eccentricity")
boxplot(raisins$ConvexArea, xlab = "ConvexArea")
boxplot(raisins$Extent, xlab = "Extent")
boxplot(raisins$Perimeter, xlab = "Perimeter")
boxplot(raisins$Class, xlab = "Class")
```

### Histograms highlighting class differences

(Aim: See more clearly if variables' distribution depend on Class)

```{r}
library(ggthemes)
```

Black: Kecimen, Yellow: Besni

```{r, echo=FALSE}
theme_set(theme_economist())

ggplot() +
geom_histogram(data = subset(x=raisins, subset=Class==1), 
               aes(x = Area), fill = 'black', alpha = 0.5) +
geom_histogram(data = subset(x=raisins, subset=Class==0), 
                 aes(x = Area), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Area's distribution" )) 


ggplot() + 
geom_histogram(data = subset(x=raisins, subset=Class==1), 
               aes(x = MinorAxisLength), fill = 'black', alpha = 0.5) + 
geom_histogram(data = subset(x=raisins, subset=Class==0), 
               aes(x = MinorAxisLength), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between MinorAxisLength's distribution" )) 


ggplot() + 
geom_histogram(data = subset(x=raisins, subset=Class==1), 
               aes(x = Eccentricity), fill = 'black', alpha = 0.5) + 
geom_histogram(data = subset(x=raisins, subset=Class==0), 
               aes(x = Eccentricity), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Eccentricity's distribution" )) 

ggplot() + 
geom_histogram(data = subset(x=raisins, subset=Class==1), 
               aes(x = ConvexArea), fill = 'black', alpha = 0.5) + 
geom_histogram(data = subset(x=raisins, subset=Class==0), 
               aes(x = ConvexArea), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between ConvexArea's distribution" )) 


ggplot() + 
geom_histogram(data = subset(x=raisins, subset=Class==1), 
               aes(x = Extent), fill = 'black', alpha = 0.5) + 
geom_histogram(data = subset(x=raisins, subset=Class==0), 
               aes(x = Extent), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Extent's distribution" ))

ggplot() + 
geom_histogram(data = subset(x=raisins, subset=Class==1), 
               aes(x = Perimeter), fill = 'black', alpha = 0.5) + 
geom_histogram(data = subset(x=raisins, subset=Class==0), 
               aes(x = Perimeter), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Perimeter's distribution" ))


```

# Supervised models

#### Splitting train and test test

```{r}
library(caret)

set.seed(42)
training_index = createDataPartition(raisins$Class, p=0.7, list = FALSE) # index of the train set examples
train = raisins[training_index,]
test = raisins[-training_index,]

```

#### MSE Function

(Aim: to evaluate the performance of predictive models)

```{r}
mse = function(predictions,data,y){
  residuals = (predictions - (data[c(y)]))
  mse = (1/nrow(data))*sum((residuals^2))
  return(mse)
}
```

## MODELS

### 1. OLS

```{r}
# Fit the linear regression model
ols = lm("Class ~ .",data=train)
# Summary of the model
print(summary(ols))
```

```{r}
# Predictions on the test data
ols_test_predictions = predict.lm(ols,newdata = test)
# Histogram of the fitted values
hist(fitted(ols))
# Calculate MSE for the training data
mse_train<-mse(fitted(ols), train, "Class") #training error
mse_train
# Calculate MSE for the test data
mse_test<-mse(ols_test_predictions,test,"Class") #test error
mse_test
```

### 2. ROBUST OLS

```{r}
library(sandwich)
library(lmtest)
library(MASS)

# Fit the robust linear regression model
ols_robust = rlm(Class ~ ., data = train, se_type = "HC2")
# Summary of the robust model
summary(ols_robust)
# Predictions on the test data
ols_robust_test_predictions = predict(ols_robust, newdata = test)
# Histogram of the fitted values
hist(fitted(ols_robust))
# Calculate MSE for the training data
mse(fitted(ols_robust), train, "Class") #training error
# Calculate MSE for the test data
mse(ols_robust_test_predictions, test, "Class") #test error
```

### 3. LOGISTIC

```{r}
library(broom)

# 3. Logistic
logistic = glm(Class ~ ., data = train, family = binomial(link = 'logit'))
tidy(logistic)
hist(fitted(logistic))
#logistic by hand
logistic_test_predictions = predict(logistic, newdata = test)
mse(fitted(logistic), train, "Class")
mse(logistic_test_predictions, test, "Class")
```

### 4. RIDGE

```{r, echo=FALSE}
library(glmnet)
X = model.matrix(Class~.-1, data = train)
y=train$Class
ridge=glmnet(X,y,alpha=0)
#ridge$beta
plot(ridge,xvar="lambda", label = TRUE) # Extent(4) and Eccentricity (6) are the variables kept
ridge_fitted = predict(ridge, newx = X) # fitted value for the training set using the best lambda value automatically selected by the function
ridge_predicted = predict(ridge, newx = model.matrix(Class~.-1, data = test)) # fitted value for the training set using the best lambda value automatically selected by the function
cv.ridge=cv.glmnet(X,y,alpha=0)
coef(cv.ridge)
plot(cv.ridge) # cv mse of the ridge
cv.ridge_predicted = predict(cv.ridge, newx = X)
mse(ridge_fitted, train, "Class") # training error of the ridge
mse(ridge_predicted, test, "Class") # test error of the ridge
mse(cv.ridge_predicted, test, "Class") # cv test error of the ridge
```

### 5. LASSO

```{r, echo=FALSE}
# Fit the lasso regression model
fit.lasso=glmnet(X,y)
# Plot the lasso coefficients
plot(fit.lasso,xvar="lambda",label=TRUE)
# Perform cross-validation for lasso regression
cv.lasso=cv.glmnet(X,y)
# Extract the coefficients from cross-validation
plot(cv.lasso)
coef(cv.lasso)
# Calculate mean squared error for the training data
#mse(fit.lasso, raisins, "Class")
##Predict using the lasso regression model
#predict(fit.lasso,newx = X)
#
```

### 6. TREE

```{r, echo=FALSE}
library(tree)
# Fit the decision tree model
tree = tree(Class ~ ., data = raisins)
# Plot the decision tree
plot(tree)
text(tree)

tree_test_predictions = predict(tree, newdata = test, type = "tree")

# Make predictions on the training data
tree_predictions = predict(tree, newdata = train[,-8], type = "tree")

# Calculate mean squared error for the test data
#mse(tree_test_predictions, test, "Class")

#plot carino
#valutarlo : prendere predizioni e calcolare mse su training e test
#confusion matrix
```

### 7. KNN

```{r, echo=FALSE}
# Scaling
train.array <- scale(train[, 1:7])
test.array<- scale(test[, 1:7])
# Labels
training_labels=train$Class
# KNN Model 
k_values <- 1:50
accuracy_scores <- numeric(length(k_values))

for (i in seq_along(k_values)) {
  #KNN model
  classifier_knn <- knn(train, test, cl = training_labels, k = k_values[i])
  
  # Convert to factors
  classifier_knn <- as.factor(classifier_knn)
  actual_labels <- as.factor(test$Class)
  
  #Accuracy
  cm <- confusionMatrix(classifier_knn, actual_labels)
  accuracy_scores[i] <- cm$overall["Accuracy"]
}

# Best K value 
best_k <- k_values[which.max(accuracy_scores)]

#print(paste("Accuracy scores:", accuracy_scores))
print(paste("Best K value:", best_k))

#Data frame with K values and accuracy
k_values_results <- data.frame(K = k_values, Accuracy = accuracy_scores)
# Plot
ggplot(k_values_results, aes(x = K, y = 1 - Accuracy)) +
  geom_line() +
  geom_point() +
  labs(x = "K", y = "Test Error") +
  ggtitle("Test Error vs K") +
  theme_minimal()
```

# Unsupervised models

### 1.PCA

```{r}
df = raisins[-c(8, 9)]
```

```{r}
options(scipen = 10)
round((apply(df, 2, mean)), digits = 5); round((apply(df, 2, var)), digits = 5)
```

```{r}
summary(df)
```

```{r}
#correlazione di ogni variabile con ogni componente
pr.out = prcomp(df, scale = TRUE)
pr.out
```

```{r}
summary(pr.out)
```

```{r}

```

```{r}
fviz_eig(pr.out, addlabels = TRUE, ylim = c(0, 70), main = "Scree Plot of PCA")
fviz_pca_var(pr.out, col.var = "blue", col.quanti.sup = "red", 
             addlabels = TRUE, repel = TRUE)
```


```{r}
plot(pr.out$x[, 1], pr.out$x[, 2], type = "n", xlab = "PC1", ylab = "PC2") 
points(pr.out$x[, 1], pr.out$x[, 2], col = rgb(1, 0, 0, alpha = 0.5), pch = 16)  
arrows(0, 0, pr.out$rotation[, 1]*7, pr.out$rotation[, 2]*7, length = 0.1, angle = 30)
text(pr.out$rotation[, 1]*7, pr.out$rotation[, 2]*7, labels = rownames(pr.out[[2]]), pos = 3)
```

```{r}
pcadf <- predict(pr.out, newdata = df)

head(pcadf)
```

### 2.Clustering su tutte le variabili

```{r}
km.out = kmeans(df, 2)

km.out
```

```{r, echo=FALSE}
#Since clusters do not correspond to a specific category, we cannot estimate accuracy. 
#However, distribution should be 450-450, but it is 189-711, so the algorithm is clearly not adequate for this dataset.
plot (df, col = adjustcolor(km.out$cluster + 1, alpha.f = 0.1),
main = "K- Means Clustering Results with K = 2", pch = 20)
```

### 3. Clustering su PCA

```{r}

#Perform 2-means clustering on the components' values for each observation
km.out2 = kmeans(pcadf, 2)
km.out2
```

```{r, echo=FALSE}
#Graphical representation of the clusters. 
#Results are slightly better on PC than on initial features
plot (pcadf, col = adjustcolor(km.out2$cluster + 1, alpha.f = 0.5),
main = "K- Means Clustering Results with K = 2", pch = 20)
```
