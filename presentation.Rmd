---
title: "Presentation"
author: "Casual deep learning"
date: "2023-06-13"
output:
  html_document:
    keep_md: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# Description

Raisins can be of two species: *Kecimen* and *Besni*.\
We built different models to predict the species on the basis of the dimensions of the raisin example approximated as an ellipse.

## Variables

The predictors that can be used are:

-   Area 
-   MajorAxisLength
-   MinorAxisLength
-   Eccentricity
-   ConvexArea
-   Extent
-   Perimeter
-   Class_literal

# Descriptive Analysis

#### Loading Libraries

```{r}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(glmnet)
library(estimatr)
library(stats)
library(maxLik)
library(Matrix)
library(caret)
library(performance)
library(see)
library(corrplot)
library(GGally)
library(car)
library(FactoMineR)
library(factoextra)
library(dplyr)
library(e1071)
library(caTools)
library(class)
library(pROC)
library(tree)
library(leaps)   # best subset
library(bestglm) # best subset for logistic
library(MASS)    # diagnostics
library(olsrr)   # diagnostics
library(influence.ME)
library(DHARMa)
```

#### Loading data-set

(Aim: visualize dataset, summary, remove Class_literal)

```{r}
raisins = read.csv(
  "https://raw.githubusercontent.com/LeonardoAcquaroli/raisins_and_mushrooms/main/datasets/Raisin_Dataset.csv",
  sep = ";"
)

head(raisins)

# remove the Class_literal  column  
raisins_corr <- raisins
raisins <- raisins[,-8] 
# Class: 1 if raisin is of Kecimen type, 0 if it is Besni
```

### Correlations plots

(Aim: visually represent relationships between variables)

```{r, echo=FALSE}
cor_table<-cor(raisins) 

corrplot(cor_table, type = "upper",     #first corr plot
         tl.col = "black", tl.srt = 45)

ggcorr(raisins, method = c("everything", "pearson")) #heatmap plot

ggpairs(raisins_corr, columns = 1:7, ggplot2::aes(colour= Class_literal, alpha = 0.005)) #cor by groups
```

### Boxplot

(Aim: Identify outliers)

```{r, echo=FALSE}
boxplot(raisins$Area, xlab = "Area")
boxplot(raisins$MajorAxisLength, xlab = "MajorAxisLength")
boxplot(raisins$MinorAxisLength, xlab = "MinorAxisLength")
boxplot(raisins$Eccentricity, xlab = "Eccentricity")
boxplot(raisins$ConvexArea, xlab = "ConvexArea")
boxplot(raisins$Extent, xlab = "Extent")
boxplot(raisins$Perimeter, xlab = "Perimeter")
boxplot(raisins$Class, xlab = "Class")
```

### Histograms highlighting class differences

(Aim: See more clearly if variables' distribution depend on Class)

```{r}
library(ggthemes)
```

Black: Kecimen, Yellow: Besni

```{r, echo=FALSE}
theme_set(theme_economist())

ggplot() +
geom_histogram(data = subset(x=raisins, subset=Class==1), 
               aes(x = Area), fill = 'black', alpha = 0.5) +
geom_histogram(data = subset(x=raisins, subset=Class==0), 
                 aes(x = Area), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Area's distribution" )) 


ggplot() + 
geom_histogram(data = subset(x=raisins, subset=Class==1), 
               aes(x = MinorAxisLength), fill = 'black', alpha = 0.5) + 
geom_histogram(data = subset(x=raisins, subset=Class==0), 
               aes(x = MinorAxisLength), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between MinorAxisLength's distribution" )) 


ggplot() + 
geom_histogram(data = subset(x=raisins, subset=Class==1), 
               aes(x = Eccentricity), fill = 'black', alpha = 0.5) + 
geom_histogram(data = subset(x=raisins, subset=Class==0), 
               aes(x = Eccentricity), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Eccentricity's distribution" )) 

ggplot() + 
geom_histogram(data = subset(x=raisins, subset=Class==1), 
               aes(x = ConvexArea), fill = 'black', alpha = 0.5) + 
geom_histogram(data = subset(x=raisins, subset=Class==0), 
               aes(x = ConvexArea), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between ConvexArea's distribution" )) 


ggplot() + 
geom_histogram(data = subset(x=raisins, subset=Class==1), 
               aes(x = Extent), fill = 'black', alpha = 0.5) + 
geom_histogram(data = subset(x=raisins, subset=Class==0), 
               aes(x = Extent), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Extent's distribution" ))

ggplot() + 
geom_histogram(data = subset(x=raisins, subset=Class==1), 
               aes(x = Perimeter), fill = 'black', alpha = 0.5) + 
geom_histogram(data = subset(x=raisins, subset=Class==0), 
               aes(x = Perimeter), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Perimeter's distribution" ))


```

# Supervised models

## We first use all the dataset

#### We run a basic ols model, which would be a linear probability model
```{r}
ols <- lm("Class ~ .",data=raisins)
summary(ols)
```

#### We check for heteroscedasticity and multicollinearity
```{r}
check_heteroscedasticity(ols) # there is heteroskedasticity
check_model(ols)
vif(ols)
sqrt(vif(ols)) > 2
```
#### There appears to be heteroscedacity, plus the multicollinearity is extremely high

#### We try to tackle multicollinearity by reducing the variables in the model. We use best subset selection.
```{r, fig.align='center'}
regfit.full=regsubsets(Class ~.,data=raisins, nvmax=7)
reg.summary=summary(regfit.full)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp")
which.min(reg.summary$cp)
plot(regfit.full,scale="Cp")
```

#### As we can see, the best model appears to be one with 5 variables. We run this improved ols model
```{r}
ols_imp <- lm("Class ~ Area + MajorAxisLength + 
              Eccentricity + ConvexArea + Perimeter",data=raisins)
summary(ols_imp)
vif(ols_imp) # the VIF is still too high!
```
#### the VIF is extremely high. 
#### Given that most of our variables are highly correlated, we identify three "proper" dimensions over which our data span.
#### Thus, we perform best subset selection specifying that we want at most 3 variables
```{r, fig.align='center'}
regfit.full=regsubsets(Class ~.,data=raisins, nvmax=3)
reg.summary=summary(regfit.full)
plot(regfit.full,scale="Cp")
```

#### We observe that two variables are very similar. Thus, we opt for the minimal model with two variables.

```{r}
ols_imp2 <- lm("Class ~ Eccentricity + Perimeter",data=raisins)
summary(ols_imp2)
vif(ols_imp2)
check_model(ols_imp2)
```

#### ols_imp2 is our final model as far as linear regression goes. We now focus on its residuals.

```{r}
raw_res <- residuals(ols_imp2)
threshold <- 3 * sd(raw_res)  # Define threshold as 3 times the SD
threshold# how are the residuals distributed?
hist(raw_res, breaks = 30, main = "Histogram of Raw Residuals", xlab = "Raw Residuals")

```

#### We also compute Cook's distance and plot it with the residuals

```{r}
cooks_dist <- cooks.distance(ols_imp2)
cooks_threshold <- 4/897
plot(cooks_dist, raw_res, main = "Cook's Distance vs. Raw Residuals",
     xlab = "Cook's Distance", ylab = "Raw Residuals")
abline(h = 0, lty = 2, col = "red")  # Reference line at y = 0
abline(h = threshold, lty = 3, col = "blue")  # Threshold line for raw residuals (upper)
abline(h = -threshold, lty = 3, col = "blue")  # Threshold line for raw residuals (lower)
abline(v = cooks_threshold, lty = 3, col = "green")
text(cooks_dist, raw_res, labels = 1:length(cooks_dist), pos = 3)
```

#### The most problematic observations appear to be: 695, 507, 837, 291, 86, 488. We should dig deeper into them.
```{r, fig.align='center'}
outliers_obs <- c(695, 507, 837, 291, 86, 488)
outliers_df <- raisins[outliers_obs, ]
non_outliers_df <- raisins[-outliers_obs, ]

# Plot without outliers
p <- ggplot(non_outliers_df, aes(x = Perimeter, y = Eccentricity)) +
  geom_point() +
  labs(title = "Scatter Plot emphasising outliers",
       x = "Perimeter",
       y = "Eccentricity")

# Add outliers with distinct color
p <- p +
  geom_point(data = outliers_df, color = "red", size = 3) +
  geom_text(data = outliers_df, aes(label = paste(rownames(outliers_df))), vjust = -1)

# Median values
p <- p +
  geom_vline(aes(xintercept = median(Perimeter)), linetype = "dashed") +
  geom_hline(aes(yintercept = median(Eccentricity)), linetype = "dashed")

# Regression line
p <- p +
  geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(data = non_outliers_df, method = "lm", se = FALSE, linetype = "dashed", color = "blue")

# Display the plot
print(p)

```

#### These observations clearly have a very large "Perimeter", however they do not appear to be completely out of this world. <br> We can still try to remove them and see how the performance changes. <br> We now plot the accuracy of our model on the whole dataset



```{r}
#computation of the accuracy on the full dataset
fitvols_fulld = predict(ols_imp2, raisins) # fulld identifies the full dataset
predols_fulld = ifelse(fitvols_fulld < 0.50, 0, 1)
accols_fulld = raisins$Class == predols_fulld
table(accols_fulld)
accuracy_ols_fulld <- 776/900
accuracy_ols_fulld
```

#### Given that we found heteroscedasticity, we perform the same model with the robust option.

```{r} 
#ROBUST OLS
ols_robust <- lm_robust(Class ~ Eccentricity + Perimeter , data = raisins, se_type = "HC2")
summary(ols_robust)
```
#### We compute the accuracy as we did before
```{r}
fitvolsrob_fulld = predict(ols_robust, raisins) # fulld identifies the full dataset
predolsrob_fulld = ifelse(fitvolsrob_fulld < 0.50, 0, 1)
accolsrob_fulld = raisins$Class == predolsrob_fulld
table(accolsrob_fulld)
accuracy_olsrob_fulld <- 776/900
accuracy_olsrob_fulld

```
#### We now remove the "outliers" from the dataset and preform the very same models.

```{r}
raisins_noout <- raisins[!(row.names(raisins) %in% c(695, 507, 837, 291, 86, 488)), ]
```

#### First the "normal" model
```{r}
ols_imp2_n <- lm("Class ~ Eccentricity + Perimeter",data=raisins_noout) # _n identifies no outliers
summary(ols_imp2_n)
fitvols_fulld_n = predict(ols_imp2_n, raisins_noout) # fulld identifies the full dataset
predols_fulld_n = ifelse(fitvols_fulld_n < 0.50, 0, 1)
accols_fulld_n = raisins_noout$Class == predols_fulld_n
table(accols_fulld_n)
accuracy_ols_fulld_n <- 772/894
accuracy_ols_fulld_n
```
#### Then the robust one

```{r}
ols_rob_n <- lm("Class ~ Eccentricity + Perimeter",data=raisins_noout) # _n identifies no-outliers
summary(ols_rob_n)
fitvolsrob_fulld_n = predict(ols_rob_n, raisins_noout) # fulld identifies the full dataset
predolsrob_fulld_n = ifelse(fitvolsrob_fulld_n < 0.50, 0, 1)
accolsrob_fulld_n = raisins_noout$Class == predolsrob_fulld_n
table(accolsrob_fulld_n)
accuracy_olsrob_fulld_n <- 772/894
accuracy_olsrob_fulld_n
```


### We now move to logistic regression

```{r, fig.align='center'}
logistic_model <-  glm(Class ~ ., data = raisins, family = binomial(link = 'logit'))
tidy(logistic_model)
vif(logistic_model)
hist(fitted(logistic_model))
```

#### We perform best subset selection on the logistic model as well
```{r}
bestglm(raisins, family = gaussian, IC = "BIC")
imp_logistic_model <- glm(Class ~ Area + ConvexArea + Perimeter, 
                    data = raisins, 
                    family = binomial(link = 'logit'))
vif(imp_logistic_model)
```

####  These still appears to have multicollinearity issues.
#### As before, we use our intuition to build a minimal logistic model
#### We actually build two of them:

```{r}
minimal1_logistic_model <-  glm(Class ~ Extent + Eccentricity, data = raisins, family = binomial(link = 'logit'))
vif(minimal1_logistic_model)

minimal2_logistic_model <-  glm(Class ~ Eccentricity + Perimeter, data = raisins,family = binomial(link = 'logit'))
vif(minimal2_logistic_model)
```

#### We now create a table with the accuracies of all our models
```{r}
accuracies <- data.frame(Model = character(),
                         Accuracy = numeric())

models <- list(logistic = logistic_model, 
               imp_logistic = imp_logistic_model,
               minimal_logit_extent = minimal1_logistic_model,
               minimal_logit_perimeter = minimal2_logistic_model,
               lpm = ols_imp2,
               lpm_rob = ols_robust)

for (model_name in names(models)) {
  model <- models[[model_name]]
  
  # Predict the outcome probabilities using the logistic regression model
  pred_pr <- predict(model, raisins, type = "response")
  # Convert the predicted probabilities to binary predictions (0 or 1)
  pred_val <- ifelse(pred_pr > 0.5, 1, 0)
  # Compare the predicted values with the actual outcome variable
  actual_values <- raisins$Class
  # Compute the number of correctly predicted values
  correctly_pred <- sum(pred_val == actual_values)
  # Compute the percentage of correctly predicted values
  accuracy <- correctly_pred / length(actual_values) * 100
  # Append the accuracy to the "accuracies" data frame
  accuracies <- rbind(accuracies, data.frame(Model = model_name, Accuracy = accuracy))
}

accuracies
```

#### Since the minimal logisitc model with the perimeter is the best one, we compute diagnostics on it.
```{r}
predicted_probs_logit <- predict(minimal2_logistic_model, type = "response")
residuals_logit <- raisins$Class - predicted_probs_logit

plot(predicted_probs_logit, raisins$Class, xlab = "Predicted Probabilities", ylab = "Observed Responses",
     main = "Observed vs. Predicted Probabilities", pch = 16)

#check relations with each predictor
plot(raisins$Eccentricity, residuals_logit, xlab = "Eccentricity", ylab = "Standardized Residuals",
     main = "Standardized Residuals vs. Predictor 1", pch = 16)
plot(raisins$Perimeter, residuals_logit, xlab = "Perimeter", ylab = "Standardized Residuals",
     main = "Standardized Residuals vs. Predictor 1", pch = 16)

# Diagnostics with DHARMa

# Create a simulated residuals object
simulated_residuals <- simulateResiduals(minimal2_logistic_model, n = 100)
# Plot standardized residuals using DHARMa's built-in diagnostic plots
plot(simulated_residuals)
```

### Comparison of all the models so far

```{r}
# Compare all the models
compare_performance(ols_imp2, ols_robust, minimal2_logistic_model)
plot(compare_performance(ols_imp2, minimal2_logistic_model, rank = TRUE, verbose = FALSE))

accuracies
```


## We now performe some actual statistical learning
#### We split our data in train and test set

```{r}
library(caret)

set.seed(42)
training_index = createDataPartition(raisins$Class, p=0.7, list = FALSE) # index of the train set examples
train = raisins[training_index,]
test = raisins[-training_index,]

```

#### MSE Function

(Aim: to evaluate the performance of predictive models)

```{r}
mse = function(predictions,data,y){
  residuals = (predictions - (data[c(y)]))
  mse = (1/nrow(data))*sum((residuals^2))
  return(mse)
}
```

## MODELS

### 1. OLS

```{r}
# Fit the linear regression model
ols = lm("Class ~ Perimeter + Eccentricity",data=train)
# Summary of the model
print(summary(ols))
```

```{r}
# Predictions on the test data
ols_test_predictions = predict.lm(ols,newdata = test)
# Histogram of the fitted values
hist(fitted(ols))
# Calculate MSE for the training data
mse_train<-mse(fitted(ols), train, "Class") #training error
mse_train
# Calculate MSE for the test data
mse_test<-mse(ols_test_predictions,test,"Class") #test error
mse_test # it was 0.1346953 before changing variables # it's 0.145 after reducing the model
```

### 2. ROBUST OLS

```{r}
library(sandwich)
library(lmtest)
library(MASS)

# Fit the robust linear regression model
ols_robust = rlm(Class ~ ., data = train, se_type = "HC2")
# Summary of the robust model
summary(ols_robust)
# Predictions on the test data
ols_robust_test_predictions = predict(ols_robust, newdata = test)
# Histogram of the fitted values
hist(fitted(ols_robust))
# Calculate MSE for the training data
mse(fitted(ols_robust), train, "Class") #training error
# Calculate MSE for the test data
mse(ols_robust_test_predictions, test, "Class") #test error
```

### 3. LOGISTIC

```{r}
library(broom)

# 3. Logistic
logistic = glm(Class ~ Perimeter + Eccentricity, data = train, family = binomial(link = 'logit'))
tidy(logistic)
hist(fitted(logistic))
#logistic by hand
logistic_test_predictions = predict(logistic, newdata = test)
mse(fitted(logistic), train, "Class") # 0.09080575 before <- 0.09835077 after 
mse(logistic_test_predictions, test, "Class")
```

### 4. RIDGE

```{r}
library(glmnet)
X = model.matrix(Class~.-1, data = train)
y=train$Class
ridge=glmnet(X,y,alpha=0)
#ridge$beta
plot(ridge,xvar="lambda", label = TRUE) # Extent(4) and Eccentricity (6) are the variables kept
ridge_fitted = predict(ridge, newx = X) # fitted value for the training set using the best lambda value automatically selected by the function
ridge_predicted = predict(ridge, newx = model.matrix(Class~.-1, data = test)) # fitted value for the training set using the best lambda value automatically selected by the function
cv.ridge=cv.glmnet(X,y,alpha=0)
coef(cv.ridge)
plot(cv.ridge) # cv mse of the ridge
cv.ridge_predicted = predict(cv.ridge, newx = X)
mse(ridge_fitted, train, "Class") # training error of the ridge
mse(ridge_predicted, test, "Class") # test error of the ridge
mse(cv.ridge_predicted, test, "Class") # cv test error of the ridge
```

### 5. LASSO

```{r}
# Fit the lasso regression model
fit.lasso=glmnet(X,y)
# Plot the lasso coefficients
plot(fit.lasso,xvar="lambda",label=TRUE)
# Perform cross-validation for lasso regression
cv.lasso=cv.glmnet(X,y)
# Extract the coefficients from cross-validation
plot(cv.lasso)
coef(cv.lasso)
# Calculate mean squared error for the training data
#mse(fit.lasso, raisins, "Class")
##Predict using the lasso regression model
#predict(fit.lasso,newx = X)
#
```

### 6. TREE

```{r}
library(tree)
# Fit the decision tree model
tree = tree(Class ~ ., data = raisins)
# Plot the decision tree
plot(tree)
text(tree)

tree_test_predictions = predict(tree, newdata = test, type = "tree")

# Make predictions on the training data
tree_predictions = predict(tree, newdata = train[,-8], type = "tree")

# Calculate mean squared error for the test data
#mse(tree_test_predictions, test, "Class")

#plot carino
#valutarlo : prendere predizioni e calcolare mse su training e test
#confusion matrix
```

### 7. KNN

```
# Scaling
train.array <- scale(train[, 1:7])
test.array<- scale(test[, 1:7])
# Labels
training_labels=train$Class
# KNN Model 
k_values <- 1:50
accuracy_scores <- numeric(length(k_values))

for (i in seq_along(k_values)) {
  #KNN model
  classifier_knn <- knn(train, test, cl = training_labels, k = k_values[i])
  
  # Convert to factors
  classifier_knn <- as.factor(classifier_knn)
  actual_labels <- as.factor(test$Class)
  
  #Accuracy
  cm <- confusionMatrix(classifier_knn, actual_labels)
  accuracy_scores[i] <- cm$overall["Accuracy"]
}

# Best K value 
best_k <- k_values[which.max(accuracy_scores)]

#print(paste("Accuracy scores:", accuracy_scores))
print(paste("Best K value:", best_k))

#Data frame with K values and accuracy
k_values_results <- data.frame(K = k_values, Accuracy = accuracy_scores)
# Plot
ggplot(k_values_results, aes(x = K, y = 1 - Accuracy)) +
  geom_line() +
  geom_point() +
  labs(x = "K", y = "Test Error") +
  ggtitle("Test Error vs K") +
  theme_minimal()
```

# Unsupervised models

### 1.PCA

```{r}
df = raisins[-c(8, 9)]
```

```{r}
options(scipen = 10)
round((apply(df, 2, mean)), digits = 5); round((apply(df, 2, var)), digits = 5)
```

```{r}
summary(df)
```

```{r}
#correlazione di ogni variabile con ogni componente
pr.out = prcomp(df, scale = TRUE)
pr.out
```

```{r}
summary(pr.out)
```

```{r}

```

```{r}
fviz_eig(pr.out, addlabels = TRUE, ylim = c(0, 70), main = "Scree Plot of PCA")
fviz_pca_var(pr.out, col.var = "blue", col.quanti.sup = "red", 
             addlabels = TRUE, repel = TRUE)
```


```{r}
plot(pr.out$x[, 1], pr.out$x[, 2], type = "n", xlab = "PC1", ylab = "PC2") 
points(pr.out$x[, 1], pr.out$x[, 2], col = rgb(1, 0, 0, alpha = 0.5), pch = 16)  
arrows(0, 0, pr.out$rotation[, 1]*7, pr.out$rotation[, 2]*7, length = 0.1, angle = 30)
text(pr.out$rotation[, 1]*7, pr.out$rotation[, 2]*7, labels = rownames(pr.out[[2]]), pos = 3)
```

```{r}
pcadf <- predict(pr.out, newdata = df)

head(pcadf)
```

### 2.Clustering su tutte le variabili

```{r}
km.out = kmeans(df, 2)

km.out
```

```{r, echo=FALSE}
#Since clusters do not correspond to a specific category, we cannot estimate accuracy. 
#However, distribution should be 450-450, but it is 189-711, so the algorithm is clearly not adequate for this dataset.
plot (df, col = adjustcolor(km.out$cluster + 1, alpha.f = 0.1),
main = "K- Means Clustering Results with K = 2", pch = 20)
```

### 3. Clustering su PCA

```{r}

#Perform 2-means clustering on the components' values for each observation
km.out2 = kmeans(pcadf, 2)
km.out2
```

```{r, echo=FALSE}
#Graphical representation of the clusters. 
#Results are slightly better on PC than on initial features
plot (pcadf, col = adjustcolor(km.out2$cluster + 1, alpha.f = 0.5),
main = "K- Means Clustering Results with K = 2", pch = 20)
```
