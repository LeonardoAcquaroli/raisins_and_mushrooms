knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(ggplot2)
library(dplyr)
library(tidyverse)
install.packages("purrr")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
install.packages("cli")
install.packages("cli")
install.packages("cli")
install.packages("cli")
install.packages("cli")
install.packages("cli")
install.packages("cli")
install.packages("cli")
install.packages("cli")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(glmnet)
library(glmnet)
library(estimatr)
library(stats)
library(Matrix)
library(caret)
library(corrplot)
library(GGally)
library(car)
library(FactoMineR)
library(factoextra)
library(dplyr)
library(e1071)
library(e1071)
library(caTools)
library(class)
library(pROC)
raisins = read.csv(
"https://raw.githubusercontent.com/LeonardoAcquaroli/raisins_and_mushrooms/main/datasets/Raisin_Dataset.csv",
sep = ";"
)
head(raisins)
# remove the Class_literal  column
raisins_corr <- raisins
raisins <- raisins[,-8]
# Class: 1 if raisin is of Kecimen type, 0 if it is Besni
cor_table<-cor(raisins)
corrplot(cor_table, type = "upper",     #first corr plot
tl.col = "black", tl.srt = 45)
ggcorr(raisins, method = c("everything", "pearson")) #heatmap plot
ggpairs(raisins_corr, columns = 1:6, ggplot2::aes(colour= Class_literal)) #cor by groups
cor_table<-cor(raisins)
corrplot(cor_table, type = "upper",     #first corr plot
tl.col = "black", tl.srt = 45)
ggcorr(raisins, method = c("everything", "pearson")) #heatmap plot
ggpairs(raisins_corr, columns = 1:7, ggplot2::aes(colour= Class_literal)) #cor by groups
cor_table<-cor(raisins)
corrplot(cor_table, type = "upper",     #first corr plot
tl.col = "black", tl.srt = 45)
ggcorr(raisins, method = c("everything", "pearson")) #heatmap plot
ggpairs(raisins_corr, columns = 1:7, ggplot2::aes(colour= Class_literal, alpha = 0.005)) #cor by groups
head(raisins)
install.packages(c("bit", "blob", "bslib", "cachem", "callr", "chron", "cli", "colorspace", "crayon", "curl", "data.table", "dbplyr", "digest", "dplyr", "dtplyr", "evaluate", "fansi", "fastmap", "fontawesome", "forcats", "fs", "gargle", "gert", "ggeasy", "gh", "googledrive", "googlesheets4", "gtable", "haven", "highr", "hms", "htmltools", "htmlwidgets", "httpuv", "httr", "isoband", "jsonlite", "knitr", "later", "lubridate", "modelr", "openssl", "pkgbuild", "pkgdown", "processx", "profvis", "ps", "ragg", "Rcpp", "readr", "readxl", "rmarkdown", "roxygen2", "sass", "shiny", "sourcetools", "stringi", "sys", "testthat", "tibble", "tidyverse", "tinytex", "usethis", "utf8", "vctrs", "viridisLite", "vroom", "waldo", "whisker", "xml2", "yaml", "zip", "zoo"))
install.packages(c("bit", "blob", "bslib", "cachem", "callr", "chron", "cli", "colorspace", "crayon", "curl", "data.table", "dbplyr", "digest", "dplyr", "dtplyr", "evaluate", "fansi", "fastmap", "fontawesome", "forcats", "fs", "gargle", "gert", "ggeasy", "gh", "googledrive", "googlesheets4", "gtable", "haven", "highr", "hms", "htmltools", "htmlwidgets", "httpuv", "httr", "isoband", "jsonlite", "knitr", "later", "lubridate", "modelr", "openssl", "pkgbuild", "pkgdown", "processx", "profvis", "ps", "ragg", "Rcpp", "readr", "readxl", "rmarkdown", "roxygen2", "sass", "shiny", "sourcetools", "stringi", "sys", "testthat", "tibble", "tidyverse", "tinytex", "usethis", "utf8", "vctrs", "viridisLite", "vroom", "waldo", "whisker", "xml2", "yaml", "zip", "zoo"))
install.packages(c("bit", "blob", "bslib", "cachem", "callr", "chron", "cli", "colorspace", "crayon", "curl", "data.table", "dbplyr", "digest", "dplyr", "dtplyr", "evaluate", "fansi", "fastmap", "fontawesome", "forcats", "fs", "gargle", "gert", "ggeasy", "gh", "googledrive", "googlesheets4", "gtable", "haven", "highr", "hms", "htmltools", "htmlwidgets", "httpuv", "httr", "isoband", "jsonlite", "knitr", "later", "lubridate", "modelr", "openssl", "pkgbuild", "pkgdown", "processx", "profvis", "ps", "ragg", "Rcpp", "readr", "readxl", "rmarkdown", "roxygen2", "sass", "shiny", "sourcetools", "stringi", "sys", "testthat", "tibble", "tidyverse", "tinytex", "usethis", "utf8", "vctrs", "viridisLite", "vroom", "waldo", "whisker", "xml2", "yaml", "zip", "zoo"))
install.packages(c("bit", "blob", "bslib", "cachem", "callr", "chron", "cli", "colorspace", "crayon", "curl", "data.table", "dbplyr", "digest", "dplyr", "dtplyr", "evaluate", "fansi", "fastmap", "fontawesome", "forcats", "fs", "gargle", "gert", "ggeasy", "gh", "googledrive", "googlesheets4", "gtable", "haven", "highr", "hms", "htmltools", "htmlwidgets", "httpuv", "httr", "isoband", "jsonlite", "knitr", "later", "lubridate", "modelr", "openssl", "pkgbuild", "pkgdown", "processx", "profvis", "ps", "ragg", "Rcpp", "readr", "readxl", "rmarkdown", "roxygen2", "sass", "shiny", "sourcetools", "stringi", "sys", "testthat", "tibble", "tidyverse", "tinytex", "usethis", "utf8", "vctrs", "viridisLite", "vroom", "waldo", "whisker", "xml2", "yaml", "zip", "zoo"))
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages(c("cli", "digest", "dplyr", "fansi", "fastmap", "htmltools", "tibble", "utf8", "vctrs"))
install.packages(c("cli", "digest", "fastmap", "htmltools"))
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
remove.packages("cli")
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
remove.packages("clipr")
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
install.packages("clipr")
install.packages("cli")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(glmnet)
library(estimatr)
library(stats)
library(maxLik)
library(Matrix)
library(caret)
library(performance)
library(see)
library(corrplot)
library(GGally)
library(car)
library(FactoMineR)
remove.packages("htmltools")
install.packages("htmltools")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(glmnet)
library(estimatr)
library(stats)
library(maxLik)
library(Matrix)
library(caret)
library(performance)
library(see)
library(corrplot)
library(GGally)
library(car)
library(FactoMineR)
library(factoextra)
library(dplyr)
library(e1071)
library(caTools)
library(class)
library(pROC)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(glmnet)
library(estimatr)
library(stats)
library(maxLik)
library(Matrix)
library(caret)
library(performance)
library(see)
library(corrplot)
library(GGally)
library(car)
library(FactoMineR)
library(factoextra)
library(dplyr)
library(e1071)
library(caTools)
library(class)
library(pROC)
remove.packages("fastmap")
install.packages("fastmap")
remove.packages("digest")
install.packages("digest")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(glmnet)
library(estimatr)
library(stats)
library(maxLik)
library(Matrix)
library(caret)
library(performance)
library(see)
library(corrplot)
library(GGally)
library(car)
library(FactoMineR)
library(factoextra)
library(dplyr)
library(e1071)
library(caTools)
library(class)
library(pROC)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(glmnet)
library(estimatr)
library(stats)
library(maxLik)
library(Matrix)
library(caret)
library(performance)
library(see)
library(corrplot)
library(GGally)
library(car)
library(FactoMineR)
library(factoextra)
library(dplyr)
library(e1071)
library(caTools)
library(class)
library(pROC)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(glmnet)
library(estimatr)
library(stats)
library(maxLik)
library(Matrix)
library(caret)
library(performance)
library(see)
library(corrplot)
library(GGally)
library(car)
library(FactoMineR)
library(factoextra)
library(dplyr)
library(e1071)
library(caTools)
library(class)
library(pROC)
raisins = read.csv(
"https://raw.githubusercontent.com/LeonardoAcquaroli/raisins_and_mushrooms/main/datasets/Raisin_Dataset.csv",
sep = ";"
)
head(raisins)
# remove the Class_literal  column
raisins_corr <- raisins
raisins <- raisins[,-8]
# Class: 1 if raisin is of Kecimen type, 0 if it is Besni
cor_table<-cor(raisins)
corrplot(cor_table, type = "upper",     #first corr plot
tl.col = "black", tl.srt = 45)
ggcorr(raisins, method = c("everything", "pearson")) #heatmap plot
ggpairs(raisins_corr, columns = 1:7, ggplot2::aes(colour= Class_literal, alpha = 0.005)) #cor by groups
boxplot(raisins$Area, xlab = "Area")
boxplot(raisins$MajorAxisLength, xlab = "MajorAxisLength")
boxplot(raisins$MinorAxisLength, xlab = "MinorAxisLength")
boxplot(raisins$Eccentricity, xlab = "Eccentricity")
boxplot(raisins$ConvexArea, xlab = "ConvexArea")
boxplot(raisins$Extent, xlab = "Extent")
boxplot(raisins$Perimeter, xlab = "Perimeter")
boxplot(raisins$Class, xlab = "Class")
library(ggthemes)
theme_set(theme_economist())
ggplot() +
geom_histogram(data = subset(x=raisins, subset=Class==1),
aes(x = Area), fill = 'black', alpha = 0.5) +
geom_histogram(data = subset(x=raisins, subset=Class==0),
aes(x = Area), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Area's distribution" ))
ggplot() +
geom_histogram(data = subset(x=raisins, subset=Class==1),
aes(x = MinorAxisLength), fill = 'black', alpha = 0.5) +
geom_histogram(data = subset(x=raisins, subset=Class==0),
aes(x = MinorAxisLength), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between MinorAxisLength's distribution" ))
ggplot() +
geom_histogram(data = subset(x=raisins, subset=Class==1),
aes(x = Eccentricity), fill = 'black', alpha = 0.5) +
geom_histogram(data = subset(x=raisins, subset=Class==0),
aes(x = Eccentricity), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Eccentricity's distribution" ))
ggplot() +
geom_histogram(data = subset(x=raisins, subset=Class==1),
aes(x = ConvexArea), fill = 'black', alpha = 0.5) +
geom_histogram(data = subset(x=raisins, subset=Class==0),
aes(x = ConvexArea), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between ConvexArea's distribution" ))
ggplot() +
geom_histogram(data = subset(x=raisins, subset=Class==1),
aes(x = Extent), fill = 'black', alpha = 0.5) +
geom_histogram(data = subset(x=raisins, subset=Class==0),
aes(x = Extent), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Extent's distribution" ))
ggplot() +
geom_histogram(data = subset(x=raisins, subset=Class==1),
aes(x = Perimeter), fill = 'black', alpha = 0.5) +
geom_histogram(data = subset(x=raisins, subset=Class==0),
aes(x = Perimeter), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Perimeter's distribution" ))
library(caret)
set.seed(42)
training_index = createDataPartition(raisins$Class, p=0.7, list = FALSE) # index of the train set examples
train = raisins[training_index,]
test = raisins[-training_index,]
mse = function(predictions,data,y){
residuals = (predictions - (data[c(y)]))
mse = (1/nrow(data))*sum((residuals^2))
return(mse)
}
# Fit the linear regression model
ols = lm("Class ~ .",data=train)
# Summary of the model
print(summary(ols))
# Predictions on the test data
ols_test_predictions = predict.lm(ols,newdata = test)
# Histogram of the fitted values
hist(fitted(ols))
# Calculate MSE for the training data
mse_train<-mse(fitted(ols), train, "Class") #training error
mse_train
# Calculate MSE for the test data
mse_test<-mse(ols_test_predictions,test,"Class") #test error
mse_test
library(sandwich)
library(lmtest)
library(MASS)
# Fit the robust linear regression model
ols_robust = rlm(Class ~ ., data = train, se_type = "HC2")
# Summary of the robust model
summary(ols_robust)
# Predictions on the test data
ols_robust_test_predictions = predict(ols_robust, newdata = test)
# Histogram of the fitted values
hist(fitted(ols_robust))
# Calculate MSE for the training data
mse(fitted(ols_robust), train, "Class") #training error
# Calculate MSE for the test data
mse(ols_robust_test_predictions, test, "Class") #test error
library(broom)
# 3. Logistic
logistic = glm(Class ~ ., data = train, family = binomial(link = 'logit'))
tidy(logistic)
hist(fitted(logistic))
#logistic by hand
logistic_test_predictions = predict(logistic, newdata = test)
mse(fitted(logistic), train, "Class")
mse(logistic_test_predictions, test, "Class")
library(glmnet)
X = model.matrix(Class~.-1, data = train)
y=train$Class
ridge=glmnet(X,y,alpha=0)
#ridge$beta
plot(ridge,xvar="lambda", label = TRUE) # Extent(4) and Eccentricity (6) are the variables kept
ridge_fitted = predict(ridge, newx = X) # fitted value for the training set using the best lambda value automatically selected by the function
ridge_predicted = predict(ridge, newx = model.matrix(Class~.-1, data = test)) # fitted value for the training set using the best lambda value automatically selected by the function
cv.ridge=cv.glmnet(X,y,alpha=0)
coef(cv.ridge)
plot(cv.ridge) # cv mse of the ridge
cv.ridge_predicted = predict(cv.ridge, newx = X)
mse(ridge_fitted, train, "Class") # training error of the ridge
mse(ridge_predicted, test, "Class") # test error of the ridge
mse(cv.ridge_predicted, test, "Class") # cv test error of the ridge
# Fit the lasso regression model
fit.lasso=glmnet(X,y)
# Plot the lasso coefficients
plot(fit.lasso,xvar="lambda",label=TRUE)
# Perform cross-validation for lasso regression
cv.lasso=cv.glmnet(X,y)
# Extract the coefficients from cross-validation
plot(cv.lasso)
coef(cv.lasso)
# Calculate mean squared error for the training data
#mse(fit.lasso, raisins, "Class")
##Predict using the lasso regression model
#predict(fit.lasso,newx = X)
#
library(tree)
# Fit the decision tree model
tree = tree(Class ~ ., data = raisins)
# Plot the decision tree
plot(tree)
text(tree)
tree_test_predictions = predict(tree, newdata = test, type = "tree")
# Make predictions on the training data
tree_predictions = predict(tree, newdata = train[,-8], type = "tree")
# Calculate mean squared error for the test data
#mse(tree_test_predictions, test, "Class")
#plot carino
#valutarlo : prendere predizioni e calcolare mse su training e test
#confusion matrix
# Scaling
train.array <- scale(train[, 1:7])
test.array<- scale(test[, 1:7])
# Labels
training_labels=train$Class
# KNN Model
k_values <- 1:50
accuracy_scores <- numeric(length(k_values))
for (i in seq_along(k_values)) {
#KNN model
classifier_knn <- knn(train, test, cl = training_labels, k = k_values[i])
# Convert to factors
classifier_knn <- as.factor(classifier_knn)
actual_labels <- as.factor(test$Class)
#Accuracy
cm <- confusionMatrix(classifier_knn, actual_labels)
accuracy_scores[i] <- cm$overall["Accuracy"]
}
# Best K value
best_k <- k_values[which.max(accuracy_scores)]
#print(paste("Accuracy scores:", accuracy_scores))
print(paste("Best K value:", best_k))
#Data frame with K values and accuracy
k_values_results <- data.frame(K = k_values, Accuracy = accuracy_scores)
# Plot
ggplot(k_values_results, aes(x = K, y = 1 - Accuracy)) +
geom_line() +
geom_point() +
labs(x = "K", y = "Test Error") +
ggtitle("Test Error vs K") +
theme_minimal()
df = raisins[-c(8, 9)]
options(scipen = 10)
round((apply(df, 2, mean)), digits = 5); round((apply(df, 2, var)), digits = 5)
summary(df)
#correlazione di ogni variabile con ogni componente
pr.out = prcomp(df, scale = TRUE)
pr.out
summary(pr.out)
fviz_eig(pr.out, addlabels = TRUE, ylim = c(0, 70), main = "Scree Plot of PCA")
fviz_pca_var(pr.out, col.var = "blue", col.quanti.sup = "red",
addlabels = TRUE, repel = TRUE)
#Correct dimensions, show clearly the points but it is not readable
#plot(pr.out$x[, 1], pr.out$x[, 2], type = "n", xlab = "PC1", ylab = "PC2")
#points(pr.out$x[, 1], pr.out$x[, 2], col = rgb(1, 0, 0, alpha = 0.5), pch = 16)
#arrows(0, 0, pr.out$rotation[, 1], pr.out$rotation[, 2], length = 0.1, angle = 30)
#Shows both the dimension and the arrows' label, but not the points
biplot(pr.out)
#Compromise, arrows length increased
plot(pr.out$x[, 1], pr.out$x[, 2], type = "n", xlab = "PC1", ylab = "PC2")
points(pr.out$x[, 1], pr.out$x[, 2], col = rgb(1, 0, 0, alpha = 0.5), pch = 16)
arrows(0, 0, pr.out$rotation[, 1]*7, pr.out$rotation[, 2]*7, length = 0.1, angle = 30)
text(pr.out$rotation[, 1]*7, pr.out$rotation[, 2]*7, labels = rownames(pr.out[[2]]), pos = 3)
pcadf <- predict(pr.out, newdata = df)
head(pcadf)
km.out = kmeans(df, 2)
km.out
#Since clusters do not correspond to a specific category, we cannot estimate accuracy.
#However, distribution should be 450-450, but it is 189-711, so the algorithm is clearly not adequate for this dataset.
plot (df, col = adjustcolor(km.out$cluster + 1, alpha.f = 0.1),
main = "K- Means Clustering Results with K = 2", pch = 20)
#Perform 2-means clustering on the components' values for each observation
km.out2 = kmeans(pcadf, 2)
km.out2
#Graphical representation of the clusters.
#Results are slightly better on PC than on initial features
plot (pcadf, col = adjustcolor(km.out2$cluster + 1, alpha.f = 0.5),
main = "K- Means Clustering Results with K = 2", pch = 20)
